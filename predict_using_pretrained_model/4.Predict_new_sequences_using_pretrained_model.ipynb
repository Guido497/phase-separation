{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4dc007f",
   "metadata": {},
   "source": [
    "# Step 1: Import packages and define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb09995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with loading the required packages\n",
    "import os\n",
    "import ntpath\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "from scipy import signal\n",
    "from iupred2a import iupred2a\n",
    "from Bio.SeqUtils.ProtParam import ProteinAnalysis\n",
    "from tqdm.auto import tqdm\n",
    "from iupred2a import iupred2a\n",
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "\n",
    "from multiprocessing import Pool\n",
    "# Windows\n",
    "CWD = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b5c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To load compressed pickle file\n",
    "def decompress_pickle(file):\n",
    " data = bz2.BZ2File(file, 'rb')\n",
    " data = cPickle.load(data)\n",
    " return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85550311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle a file and compress it into a file (to output data if required)\n",
    "def compressed_pickle(title, data):\n",
    " with bz2.BZ2File(title + '.pbz2', 'w') as f:cPickle.dump(data, f)\n",
    "# will be done using\n",
    "# compressed_pickle('output_name', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8dbaceeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define amino acids and their hydrophobicity index\n",
    "RESIDUES = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
    "            'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "# Kyte & Doolittle {kd} index of hydrophobicity\n",
    "HP = {'A': 1.8, 'R':-4.5, 'N':-3.5, 'D':-3.5, 'C': 2.5,\n",
    "      'Q':-3.5, 'E':-3.5, 'G':-0.4, 'H':-3.2, 'I': 4.5,\n",
    "      'L': 3.8, 'K':-3.9, 'M': 1.9, 'F': 2.8, 'P':-1.6,\n",
    "      'S':-0.8, 'T':-0.7, 'W':-0.9, 'Y':-1.3, 'V': 4.2, 'U': 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb31498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classes and functions\n",
    "import time\n",
    "class IuPred:\n",
    "    def __init__(self, glob, short, long):\n",
    "        self.glob = glob\n",
    "        self.short = short\n",
    "        self.long = long\n",
    "\n",
    "\n",
    "class HydroPhobicIndex:\n",
    "    def __init__(self, hpilist):\n",
    "        self.hpilist = hpilist\n",
    "        \n",
    "def calc_idrpred(args):\n",
    "    idx, seq = args\n",
    "    glob = iupred2a.iupred(str(seq), 'glob')\n",
    "    short = iupred2a.iupred(str(seq), 'short')\n",
    "    long = iupred2a.iupred(str(seq), 'long')\n",
    "    idrpred = IuPred(glob, short, long)\n",
    "    return idx, idrpred\n",
    "        \n",
    "\n",
    "class MakeMatrix:\n",
    "    def __init__(self, dbfasta):  \n",
    "        self.df = pd.DataFrame()\n",
    "        executables = [\n",
    "             'self.fasta2df(dbfasta)',\n",
    "             'self.amino_acid_analysis()',\n",
    "             'self.idr_iupred()',\n",
    "             'self.hydrophobic()',\n",
    "             'self.add_iupred_features()',\n",
    "             'self.add_hydrophobic_features()',\n",
    "             'self.add_biochemical_combinations()',\n",
    "             'self.add_lowcomplexity_features()' ,\n",
    "#             #'self.add_plaac()'\n",
    "        ]        \n",
    "        for e in executables:\n",
    "            start = time.time()     \n",
    "            print(e)\n",
    "            exec(e)\n",
    "            end = time.time()        \n",
    "            print(str(round(end - start, 2))+'s '+e)\n",
    "\n",
    "    def fasta2df(self, dbfasta):\n",
    "        rows = list()\n",
    "        with open(dbfasta) as f:\n",
    "            for record in SeqIO.parse(dbfasta, 'fasta'):\n",
    "                seqdict = dict()\n",
    "                seq = str(record.seq)\n",
    "                id = record.description.split('|')                \n",
    "                if id[0] == 'sp':\n",
    "                    uniprot_id = id[1]\n",
    "                    name = id[2].split(' ')[0]\n",
    "                    rows.append([name, uniprot_id, seq])\n",
    "                elif id[0] == 'tr':\n",
    "                    uniprot_id = id[1]\n",
    "                    name = id[2].split(' ')[0]\n",
    "                    rows.append([name, uniprot_id, seq])\n",
    "                else:\n",
    "                    uniprot_id = id[0]\n",
    "                    name = id[2].split(' ')[0]\n",
    "                    rows.append([name, uniprot_id, seq])                    \n",
    "        self.df = pd.DataFrame(rows, columns=['protein_name', 'uniprot_id', 'sequence'])\n",
    "        print(len(self.df))\n",
    "\n",
    "    def idr_iupred(self):\n",
    "        self.df['iupred'] = object\n",
    "        data = list(self.df[\"sequence\"].iteritems())\n",
    "        p = Pool(32)\n",
    "        for idx, idrpred in tqdm(p.imap(calc_idrpred, data), total=self.df.shape[0]):\n",
    "            self.df.at[idx, 'iupred'] = idrpred\n",
    "            \n",
    "    def hydrophobic(self):\n",
    "        for index, row in self.df.iterrows():\n",
    "            hpilst = pd.Series(list(row['sequence'])).map(HP).tolist()\n",
    "            self.df.loc[index, 'HydroPhobicIndex'] = HydroPhobicIndex(hpilst)\n",
    "            \n",
    "    def amino_acid_analysis(self):\n",
    "        for res in RESIDUES:\n",
    "            self.df['fraction_'+res] = self.df['sequence'].str.count(res) / self.df['sequence'].str.len()\n",
    "        self.df['length'] = self.df['sequence'].str.len()\n",
    "        for index, row in tqdm(self.df.iterrows(), total=self.df.shape[0]):\n",
    "        #for index, row in self.df.iterrows():\n",
    "            seq = row['sequence']   \n",
    "            seqanalysis = ProteinAnalysis(seq)\n",
    "            acidist = seqanalysis.get_amino_acids_percent() \n",
    "            self.df.loc[index, 'IEP'] = seqanalysis.isoelectric_point()\n",
    "            if 'X' not in seq and 'B' not in seq:\n",
    "                self.df.loc[index, 'molecular_weight'] = seqanalysis.molecular_weight()\n",
    "            if 'U' not in seq and 'X' not in seq and 'B' not in seq:\n",
    "                self.df.loc[index, 'gravy'] = seqanalysis.gravy()\n",
    "          \n",
    "\n",
    "    def add_iupred_features(self):\n",
    "        for index, row in tqdm(self.df.iterrows(), total=self.df.shape[0]):\n",
    "        #for index, row in self.df.iterrows():\n",
    "            idr = row['iupred'].glob[0]\n",
    "            self.df.loc[index, 'idr_percetage'] = sum(i > .5 for i in list(idr))\n",
    "            self.df.loc[index, 'idr_50'] = sum(i > .5 for i in list(idr)) / len(str(row['sequence']))\n",
    "            self.df.loc[index, 'idr_60'] = sum(i > .6 for i in list(idr)) / len(str(row['sequence']))\n",
    "            self.df.loc[index, 'idr_70'] = sum(i > .7 for i in list(idr)) / len(str(row['sequence']))\n",
    "            self.df.loc[index, 'idr_80'] = sum(i > .8 for i in list(idr)) / len(str(row['sequence']))\n",
    "            self.df.loc[index, 'idr_90'] = sum(i > .9 for i in list(idr)) / len(str(row['sequence']))\n",
    "\n",
    "    def add_hydrophobic_features(self):\n",
    "        hpi0, hpi1, hpi2, hpi3, hpi4, hpi5 = list(), list(), list(), list(), list(), list() \n",
    "        for index, row in tqdm(self.df.iterrows(), total=self.df.shape[0]):\n",
    "        #for index, row in self.df.iterrows():\n",
    "            sw = convolve_signal(row['HydroPhobicIndex'].hpilist, window=30)\n",
    "            hpi0.append(sum(i < -1.5 for i in sw) / len(sw))\n",
    "            # self.df.loc[index, 'hpi_<-1.5_frac'] = hpi\n",
    "            hpi1.append(sum(i < -2.0 for i in sw) / len(sw))\n",
    "            # self.df.loc[index, 'hpi_<-2.0_frac'] = hpi\n",
    "            hpi2.append(sum(i < -2.5 for i in sw) / len(sw))\n",
    "            # self.df.loc[index, 'hpi_<-2.5_frac'] = hpi\n",
    "            hpi3.append(sum(i < -1.5 for i in sw))\n",
    "            # self.df.loc[index, 'hpi_<-1.5'] = hpi\n",
    "            hpi4.append( sum(i < -2.0 for i in sw))\n",
    "            # self.df.loc[index, 'hpi_<-2.0'] = hpi\n",
    "            hpi5.append(sum(i < -2.5 for i in sw))\n",
    "            # self.df.loc[index, 'hpi_<-2.5'] = hpi \n",
    "        self.df['hpi_<-1.5_frac'] = hpi0\n",
    "        self.df['hpi_<-2.0_frac'] = hpi1\n",
    "        self.df['hpi_<-2.5_frac'] = hpi2\n",
    "        self.df['hpi_<-1.5'] = hpi3\n",
    "        self.df['hpi_<-2.0'] = hpi4\n",
    "        self.df['hpi_<-2.5'] = hpi5\n",
    "            \n",
    "\n",
    "    def add_biochemical_combinations(self):\n",
    "        df = self.df\n",
    "        df = df.assign(Asx=df['fraction_D'] + df['fraction_N'])\n",
    "        df = df.assign(Glx=df['fraction_E'] + df['fraction_Q'])\n",
    "        df = df.assign(Xle=df['fraction_I'] + df['fraction_L'])\n",
    "        df = df.assign(Pos_charge=df['fraction_K'] + df['fraction_R'] + df['fraction_H'])\n",
    "        df = df.assign(Neg_charge=df['fraction_D'] + df['fraction_E'])\n",
    "        df = df.assign(Aromatic=df['fraction_F'] + df['fraction_W'] + df['fraction_Y'] + df['fraction_H'])\n",
    "        df = df.assign(Alipatic=df['fraction_V'] + df['fraction_I'] + df['fraction_L'] + df['fraction_M'])\n",
    "        df = df.assign(Small=df['fraction_P'] + df['fraction_G'] + df['fraction_A'] + df['fraction_S'])\n",
    "        df = df.assign(Hydrophilic=(df['fraction_S'] + df['fraction_T'] + df['fraction_H'] + \n",
    "                                    df['fraction_N'] + df['fraction_Q'] + df['fraction_E'] +\n",
    "                                    df['fraction_D'] + df['fraction_K'] + df['fraction_R']))\n",
    "        df = df.assign(Hydrophobic= (df['fraction_V'] + df['fraction_I'] + df['fraction_L'] +\n",
    "                                     df['fraction_F'] + df['fraction_W'] + df['fraction_Y'] +\n",
    "                                     df['fraction_M']))\n",
    "        \n",
    "        # Added in version 2\n",
    "        for dimer in ['GV', 'VG', 'VP', 'PG', 'FG', 'RG', 'GR', 'GG', 'YG', 'GS', 'SG', 'GA', 'GF', 'GD', 'DS']:\n",
    "            self.df[dimer] = self.df['sequence'].str.count(dimer)\n",
    "        df = df.assign(alpha_helix=df['fraction_V'] + df['fraction_I'] + df['fraction_Y'] + df['fraction_F']\n",
    "                      + df['fraction_W'] + df['fraction_L'])\n",
    "        df = df.assign(beta_turn=df['fraction_N'] + df['fraction_P'] + df['fraction_G'] + df['fraction_S'])\n",
    "        df = df.assign(beta_sheet=df['fraction_E'] + df['fraction_M'] + df['fraction_A'] + df['fraction_L'])\n",
    "        # Calculates the aromaticity value of a protein according to Lobry, 1994. \n",
    "        # It is simply the relative frequency of Phe+Trp+Tyr.\n",
    "        df = df.assign(aromaticity=df['fraction_F'] + df['fraction_W'] + df['fraction_Y'])\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.df = df\n",
    "        del df\n",
    "        \n",
    "    def add_lowcomplexityscore(self):\n",
    "        lcs_window = 20\n",
    "        lcs_cutoff = 7\n",
    "        for index, row in self.df.iterrows():\n",
    "            seq = str(row['sequence'])\n",
    "            if len(seq) > lcs_window+1:\n",
    "                sig = list()\n",
    "                for i in range(len(seq)):\n",
    "                    window = (seq[i: i+lcs_window])\n",
    "                    if len(window) == lcs_window:\n",
    "                        acid_comp = len(list(set(window)))\n",
    "                        sig.append(acid_comp)\n",
    "                score = sum([1 if i <= 7 else 0 for i in sig])\n",
    "                self.df.loc[index, 'lcs_score'] = score\n",
    "                self.df.loc[index, 'lcs_fraction'] = score / len(sig)\n",
    "                \n",
    "                \n",
    "    def add_lowcomplexity_features(self):\n",
    "        n_window = 20\n",
    "        cutoff = 7       \n",
    "        n_halfwindow = int(n_window / 2)        \n",
    "        lcs_lowest_complexity = list()\n",
    "        lcs_scores = list()\n",
    "        lcs_fractions = list()\n",
    "        for index, row in tqdm(self.df.iterrows(), total=self.df.shape[0]):      \n",
    "            # Determine low complexity scores\n",
    "            seq = str(row['sequence'])\n",
    "            lcs_acids = list()\n",
    "            sig = list()\n",
    "            \n",
    "            # New\n",
    "            lc_bool = [False] * len(seq)\n",
    "            for i in range(len(seq)):\n",
    "                if i < n_halfwindow:\n",
    "                    peptide = seq[:n_window]        \n",
    "                elif i+n_halfwindow > int(len(seq)):\n",
    "                    peptide = seq[-n_window:]        \n",
    "                else:\n",
    "                    peptide = seq[i-n_halfwindow:i+n_halfwindow]       \n",
    "                complexity = (len(set(peptide)))\n",
    "                if complexity <= 7:\n",
    "                    for bool_index in (i-n_halfwindow, i+n_halfwindow):\n",
    "                        try:\n",
    "                            lc_bool[bool_index] = True\n",
    "                        except IndexError:\n",
    "                            pass\n",
    "                    lcs_acids.append(seq[i])\n",
    "                sig.append(complexity)            \n",
    "            # Adding low complexity scores to list\n",
    "            low_complexity_list = pd.DataFrame({'bool':lc_bool, 'acid':list(seq)}, index=None)\n",
    "            lcs_lowest_complexity.append(min(sig))\n",
    "            lcs_scores.append(len(low_complexity_list.loc[low_complexity_list['bool'] == True]))\n",
    "            lcs_fractions.append(len(low_complexity_list.loc[low_complexity_list['bool'] == True]) / len(seq))\n",
    "            low_complexity_list = pd.DataFrame({'bool':lc_bool, 'acid':list(seq)}, index=None)\n",
    "            if len(lcs_acids) >= n_window:\n",
    "                for i in RESIDUES:\n",
    "                    self.df.loc[index ,i+'_lcscore'] = (len(low_complexity_list.loc[\n",
    "                        (low_complexity_list['bool'] == True) &\n",
    "                        (low_complexity_list['acid'] == i)])\n",
    "                    )\n",
    "                    self.df.loc[index ,i+'_lcfraction'] = (len(low_complexity_list.loc[\n",
    "                        (low_complexity_list['bool'] == True) & \n",
    "                        (low_complexity_list['acid'] == i)]) / len(lcs_acids)\n",
    "                    )\n",
    "        self.df['lcs_fractions'] = lcs_fractions\n",
    "        self.df['lcs_scores'] = lcs_scores\n",
    "        self.df['lcs_lowest_complexity'] = lcs_lowest_complexity\n",
    "        \n",
    "def convolve_signal(sig, window=25):\n",
    "    win = signal.hann(window)\n",
    "    sig = signal.convolve(sig, win, mode='same') / sum(win)\n",
    "    return sig\n",
    "\n",
    "\n",
    "def average(l):\n",
    "    return sum(l) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "64276468",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(name, fasta_path, operating_system='Windows'):\n",
    "    # Change pathing\n",
    "    \"\"\" Generates and saves a file which contains features of a protein sequence in the defined output directory.\n",
    "    Parameters:\n",
    "        name: Name of the file.\n",
    "        fasta_path: Path of the fasta file which needs to be featured.\n",
    "        operating_system: String which indicates which operating system is used only 'Windows' available.\n",
    "    \"\"\"\n",
    "    data = MakeMatrix(fasta_path)   \n",
    "    #now = datetime.datetime.now()\n",
    "    #date = (str(now.day) + '-' + str(now.month)  + '-' +  str(now.year))\n",
    "    #define output directory for .pkl file here\n",
    "    #data.df.to_pickle('/output/path/'+ name + '_small_' + date + '.pkl')\n",
    "    #print('Generated file: ' + name + '_llps_f2f_' + date + '.pkl')\n",
    "    return data.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4221d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_identifiers(df, uniprot_ids, identifier_name):\n",
    "    uniprot_ids = [s.strip() for s in uniprot_ids.splitlines()]\n",
    "    df[identifier_name] = 0\n",
    "    for prot_id in uniprot_ids:\n",
    "        df.loc[df['uniprot_id'] == prot_id, identifier_name] = 1\n",
    "        if (len(df.loc[df['uniprot_id'] == prot_id])) == 0:\n",
    "            print(prot_id+' is not found.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ef812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load required packages\n",
    "# coding: utf-8\n",
    "%matplotlib inline\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "import collections\n",
    "from statistics import mean\n",
    "from tqdm import tqdm\n",
    "import pickle5 as pickle\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "# Sklearn imports\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Different machine learning models from sklearn\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "CWD = os.getcwd()\n",
    "RANDOMSTATE = 42\n",
    "\n",
    "class IuPred:\n",
    "    '''\n",
    "    Needs description.\n",
    "    '''    \n",
    "    def __init__(self, glob, short, long):\n",
    "        self.glob = glob\n",
    "        self.short = short\n",
    "        self.long = long\n",
    "\n",
    "\n",
    "class HydroPhobicIndex:\n",
    "    '''\n",
    "    Needs description.\n",
    "    '''\n",
    "    def __init__(self, hpilist):\n",
    "        self.hpilist = hpilist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66b8cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions required to train the machine learning classifier\n",
    "def preprocess_data(df, scaler):\n",
    "    info = df.select_dtypes(include=['object'])\n",
    "    y = df[instance]\n",
    "    X = df.drop([instance], axis=1)\n",
    "    X = X._get_numeric_data()\n",
    "    columns = X.columns\n",
    "    \n",
    "    X = scaler.fit_transform(X)    \n",
    "    X = pd.DataFrame(X, columns=columns)\n",
    "    X[instance] = y\n",
    "    X = X.merge(info, how='outer', left_index=True, right_index=True)    \n",
    "    return X\n",
    "\n",
    "\n",
    "def remove_correlating_features(df, cutoff=.95, plot=False):\n",
    "    # Set matplotlib parameters\n",
    "    sns.set(rc={'figure.figsize':(8,6)})\n",
    "    plt.rcParams['pdf.fonttype'] = 42\n",
    "    plt.rcParams['ps.fonttype'] = 42\n",
    "    \n",
    "    print('Correlation: Features before %s' % df.shape[1])\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr_matrix = df.corr().abs()\n",
    "\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "    # Find index of feature columns with correlation greater than 0.95\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > cutoff)]\n",
    "    # Drop features\n",
    "    df = df.drop(to_drop, axis=1)\n",
    "    \n",
    "    \n",
    "    print('Correlation: Features after %s' % df.shape[1])\n",
    "    if plot:\n",
    "        # Plot heatmap\n",
    "        sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "        sns.heatmap(data=df.corr(), cmap=sns.color_palette(\"coolwarm\", 20), vmin=-1, vmax=1)\n",
    "        plt.savefig(os.getcwd()+ '\\\\data\\graphs\\\\6.Heatmap_after_correlation.pdf', transparent=True)\n",
    "        plt.show()\n",
    "    return df\n",
    "\n",
    "\n",
    "def varianceSelection(df, threashold=.8):\n",
    "    if not isinstance(df, pd.core.frame.DataFrame):\n",
    "        logger.error('[%s] : [ERROR] Variance selection only possible on Dataframe not %s',\n",
    "                                     datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d %H:%M:%S'), type(df))\n",
    "        sys.exit(1)\n",
    "    sel = VarianceThreshold(threshold=(threashold * (1 - threashold)))\n",
    "    sel.fit_transform(df)\n",
    "    return df[[c for (s, c) in zip(sel.get_support(), df.columns.values) if s]]\n",
    "\n",
    "\n",
    "def filter_low_variance(df, non_feature_columns, variance_cutoff=.8):\n",
    "    df_variancetreshold = varianceSelection(df.drop(non_feature_columns, axis=1), variance_cutoff)\n",
    "    for col in non_feature_columns:\n",
    "        df_variancetreshold[col] = df[col]\n",
    "    return df_variancetreshold\n",
    "\n",
    "\n",
    "def remove_low_variance_features(df, variance_cutoff, plot=False):\n",
    "    print('Low varience: Features before %s' % df.shape[1])\n",
    "    non_feature_columns = ['protein_name', 'sequence', 'llps']\n",
    "    df_variance = filter_low_variance(df, non_feature_columns, variance_cutoff)\n",
    "    print('Low varience: Features after  %s' % df_variance.shape[1])\n",
    "    \n",
    "    # Set matplotlib parameters\n",
    "    if plot:\n",
    "        sns.set(rc={'figure.figsize':(8,6)})\n",
    "        plt.rcParams['pdf.fonttype'] = 42\n",
    "        plt.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "        # Plot heatmap\n",
    "        sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "        sns.heatmap(data=df_variance.corr(), cmap=sns.color_palette(\"coolwarm\", 20), vmin=-1, vmax=1)\n",
    "\n",
    "        plt.savefig(os.getcwd()+ '\\\\data\\graphs\\\\6.heatmap_after_low_variance.pdf', transparent=True)\n",
    "        plt.show()\n",
    "    return df_variance\n",
    "\n",
    "\n",
    "def get_feature_importances(df, clf):\n",
    "    '''\n",
    "    Needs description.\n",
    "    '''    \n",
    "    fi = clf.feature_importances_\n",
    "    fi = pd.DataFrame(fi).transpose()\n",
    "    fi.columns = df.columns[1:]\n",
    "    fi = fi.melt()\n",
    "    fi = fi.sort_values('value', ascending=False)\n",
    "    fi = fi.loc[fi['value'] > 0.0]\n",
    "    return fi\n",
    "\n",
    "\n",
    "def select_testing_data(df, instance, ratio=1, info_columns=['protein_name', 'sequence'], random=False):\n",
    "    '''\n",
    "    Class that equalizes the data based on the number of positives.\n",
    "    :param df: dataframe with all the features and instance\n",
    "    :param instance: string of instance columns\n",
    "    :param ratio: ratio of how many negatives should be added (2 = twice as much negative)\n",
    "    :param random: bool which gives a random test set with both instances randomly picked.\n",
    "    :return: 'out' dataframe with new data, and 'df_info' dataframe with the information of \n",
    "    protein name and sequence.\n",
    "    '''\n",
    "    pos = df.loc[df[instance] == 1]\n",
    "    neg_index = df.loc[df[instance] == 0].index\n",
    "    negsubset_index = sample(set(neg_index), len(pos) * ratio)\n",
    "    if random:\n",
    "        possubset_index = sample(set(df.index), len(pos))\n",
    "        negsubset_index = sample(set(df.index), len(pos))\n",
    "        pos = df.loc[possubset_index]\n",
    "        pos[instance] = pos[instance] = 1\n",
    "    neg = df.loc[negsubset_index]\n",
    "    out = pos.append(neg)\n",
    "    out = out.reset_index(drop=True)\n",
    "    df_info = out[info_columns]\n",
    "    out = out.drop(labels=info_columns, axis=1)\n",
    "    return out, df_info\n",
    "\n",
    "\n",
    "def calculate_performance(clf, X_test, y_test):\n",
    "    '''\n",
    "    Needs description.\n",
    "    '''    \n",
    "    prediction = clf.predict(X_test)\n",
    "    correctness = prediction == y_test\n",
    "    distribution = correctness.value_counts(True)\n",
    "    return distribution\n",
    "\n",
    "\n",
    "def train_model(data, clf, instance):\n",
    "    df, df_info = select_testing_data(data, instance, ratio=1, random=False)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df.drop(instance, axis=1),\n",
    "                                                        df[instance],\n",
    "                                                        test_size=0.5,\n",
    "                                                        random_state=RANDOMSTATE,\n",
    "                                                        )\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    return clf, X_test, y_test\n",
    "\n",
    "    \n",
    "def plot_pca(df, title='', analysis_name=''):\n",
    "    # Set matplotlib parameters\n",
    "    sns.set(rc={'figure.figsize':(8,6)})\n",
    "    plt.rcParams['pdf.fonttype'] = 42\n",
    "    plt.rcParams['ps.fonttype'] = 42\n",
    "    \n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(df.drop(['sequence', 'protein_name'],axis=1))\n",
    "    pca_df = pd.DataFrame(X_pca, columns=['component1', 'component2'])\n",
    "    pca_df['protein_name'] = df['protein_name']\n",
    "    pca_df['llps'] = df.reset_index()['llps']\n",
    "\n",
    "\n",
    "    sns.scatterplot(data=pca_df.loc[pca_df['llps'] == 0],\n",
    "                    x = 'component1',\n",
    "                    y = 'component2',\n",
    "                    alpha = .5,\n",
    "                    color = '#AEB6BF',\n",
    "                    rasterized=True,\n",
    "                   )\n",
    "    sns.scatterplot(data=pca_df.loc[pca_df['llps'] == 1],\n",
    "                    x = 'component1',\n",
    "                    y = 'component2',\n",
    "                    alpha = 1,\n",
    "                    color = '#CD6155',                \n",
    "                   )\n",
    "    \n",
    "    \n",
    "    # print(pca_df.loc[(pca_df['llps'] == 1) & (pca_df['component1'] >= 650)])\n",
    "    \n",
    "    plt.ylabel('Component1 (' + str(round(pca.explained_variance_ratio_[0] * 100, 2)) + '%)')\n",
    "    plt.xlabel('Component2 (' + str(round(pca.explained_variance_ratio_[1] * 100, 2)) + '%)')\n",
    "    plt.title(title)\n",
    "    plt.savefig(os.getcwd()+ f'\\\\analysis\\\\{analysis_name}\\\\PCA_plot.pdf', transparent=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def average(l):\n",
    "    return sum(l) / len(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0fa9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_train_indexes(data, label , ratio=1, randomized=False):\n",
    "    '''\n",
    "    Function: Will oversample the positive data with randomly selected negative samples.\n",
    "    Returns: List with indexes which contain positive and negative samples.\n",
    "    '''\n",
    "    positive_instances = set(data.loc[data[label] == 1].index)\n",
    "    negative_instances = set(data.loc[data[label] == 0].index)\n",
    "    n_positives = data.loc[data[label] == 1].shape[0]    \n",
    "    indexes = list()    \n",
    "    while len(negative_instances) >= 1:    \n",
    "        if len(negative_instances) > n_positives * ratio:\n",
    "            sample_set = sample(negative_instances, (n_positives * ratio))\n",
    "        else:\n",
    "            sample_set = list(negative_instances)\n",
    "            size = (len(sample_set))\n",
    "            short = (len(positive_instances ) * ratio) - size        \n",
    "            shortage = sample(set(data.loc[data[label] == 0].index), short)\n",
    "            sample_set = (sample_set+ shortage)\n",
    "        indexes.append((list(positive_instances) + list(sample_set)))\n",
    "        negative_instances.difference_update(set(sample_set))\n",
    "    return(indexes)\n",
    "\n",
    "\n",
    "def evaluate_model(data, label, index_list, names, classifiers, testing_size, randomize=False):\n",
    "    evaluations=dict()\n",
    "    for name, clf in zip(names, classifiers):   \n",
    "        scores=list()   \n",
    "        non_numeric_columns = (list(data.select_dtypes(include='object')))\n",
    "        data = data.select_dtypes([np.number])\n",
    "        print(f'Now training: {name}')\n",
    "        for index in tqdm(index_list):\n",
    "            # Get index values.\n",
    "            df = data.loc[index]\n",
    "            \n",
    "            # Randomize\n",
    "            if randomize:\n",
    "                pass\n",
    "                \n",
    "            # Specify data and labels.\n",
    "            X = df.drop(instance, axis=1)\n",
    "            y = df[label]\n",
    "\n",
    "            # split data and train model.\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testing_size)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # get predictions\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_proba = clf.predict_proba(X_test)\n",
    "            scores.append([list(y_test), y_pred, y_proba])        \n",
    "        evaluations[name] = scores        \n",
    "    return evaluations\n",
    "\n",
    "\n",
    "def get_model_evaluations(processed_data, ratio=1, testing_size=.1, label='llps'):\n",
    "    names = [\n",
    "        \"Random Forest\",\n",
    "        \"Gradient Boosting\",\n",
    "        \"Extra Trees\",\n",
    "        \"Decision Tree\",\n",
    "        \"KNeighbors\",\n",
    "        \"C-Support Vector\",\n",
    "        \"Gaussian Process\",\n",
    "        \"Neural Net\",\n",
    "        \"AdaBoost\",\n",
    "        \"Gaussian Naive Bayes\",\n",
    "    ]\n",
    "\n",
    "    classifiers = [\n",
    "        RandomForestClassifier(),\n",
    "        GradientBoostingClassifier(),\n",
    "        ExtraTreesClassifier(),\n",
    "        DecisionTreeClassifier(),\n",
    "        KNeighborsClassifier(),\n",
    "        SVC(probability=True),\n",
    "        GaussianProcessClassifier(1.0 * RBF(1.0), max_iter_predict=100),\n",
    "        MLPClassifier(),\n",
    "        AdaBoostClassifier(),\n",
    "        GaussianNB(),\n",
    "    ]\n",
    "\n",
    "    data = processed_data.copy()\n",
    "    index_list = get_test_train_indexes(data, label, ratio=1)\n",
    "\n",
    "    # evaluate models.\n",
    "    evaluations = evaluate_model(data, label, index_list, names, classifiers, testing_size)\n",
    "    return evaluations\n",
    "\n",
    "# data preprocessing\n",
    "def preprocess_and_scaledata(data, instance):\n",
    "    try:\n",
    "        data = data.drop(['iupred', 'HydroPhobicIndex', 'uniprot_id', 'PRDaa'], axis=1)\n",
    "    except KeyError:\n",
    "        data = data.drop(['iupred', 'HydroPhobicIndex', 'uniprot_id'], axis=1)\n",
    "    data = data.fillna(value=0)\n",
    "    print(data.shape)\n",
    "    print('Number of phase separating proteins in dataset: '+str(data.loc[data[instance] == 1].shape[0]))\n",
    "    print(data.shape)\n",
    "    \n",
    "    # scaler = QuantileTransformer(output_distribution='normal')\n",
    "    scaler = QuantileTransformer()\n",
    "    df = data.copy()\n",
    "    processed_data = df.fillna(0)\n",
    "    processed_data = preprocess_data(processed_data, scaler)\n",
    "    #processed_data = remove_correlating_features(processed_data, cutoff=.95)\n",
    "    #processed_data = remove_low_variance_features(processed_data, variance_cutoff=0.08)\n",
    "    return processed_data\n",
    "\n",
    "def split_dataframe_data(data, label, n=30, n_negative=30):\n",
    "    positive_label_sample = data[data[label] == 1].sample(n)\n",
    "    negative_label_sample = data[data[label] == 0].sample(n_negative)\n",
    "    train_data = data.drop(list(positive_label_sample.index))\n",
    "    train_data = train_data.drop(list(negative_label_sample.index))\n",
    "    test_data = shuffle(positive_label_sample.append(negative_label_sample)).reset_index(drop=True)      \n",
    "    return train_data, test_data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ce5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnsembleRandomForest:\n",
    "    def __init__(self):\n",
    "        self.estimators = list()\n",
    "\n",
    "    def fit(self, data, index_list, label):\n",
    "        estimators=list()   \n",
    "        non_numeric_columns = (list(data.select_dtypes(include='object')))\n",
    "        data = data.select_dtypes([np.number])\n",
    "        for index in tqdm(index_list):\n",
    "            df = data.loc[index]\n",
    "            X = df.drop(instance, axis=1)\n",
    "            y = df[label]\n",
    "            clf = RandomForestClassifier()\n",
    "            clf.fit(X, y)\n",
    "            estimators.append(clf)\n",
    "        self.estimators = estimators\n",
    "\n",
    "    def predict_proba(self, df, label):\n",
    "        probabilities = list()\n",
    "        for clf in self.estimators:\n",
    "            probability = clf.predict_proba(df)[:,1]\n",
    "            probabilities.append(probability)\n",
    "        probabilities = np.array(probabilities)\n",
    "        predictions = list()\n",
    "        for i in range(probabilities.shape[1]):\n",
    "            prediction = np.average(probabilities[:,i])\n",
    "            predictions.append(prediction)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92518711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proteome(df, clf, instance, testing_size, feature_imp=True, remove_training=False, second_df=pd.DataFrame()):\n",
    "    pd.set_option('mode.chained_assignment', None)    \n",
    "    prediction = df.select_dtypes(include='object')\n",
    "    df = df.select_dtypes([np.number])\n",
    "    if len(second_df) > 0:\n",
    "        prediction = second_df.select_dtypes(include='object')\n",
    "        second_df = second_df.select_dtypes([np.number]) \n",
    "    indexes = get_test_train_indexes(df, instance)\n",
    "    count = 0  \n",
    "    fi_data = None\n",
    "    for index in tqdm(indexes):\n",
    "        df_fraction = df.loc[index]        \n",
    "    # Also consider X_test index for prediction in the proteome    \n",
    "        X = df_fraction.drop(instance, axis=1)        \n",
    "        y = df_fraction[instance]\n",
    "        clf.fit(X, y)           \n",
    "    # Feature importance\n",
    "        if feature_imp:        \n",
    "            #X = df_fraction.drop('llps', axis=1)\n",
    "            fi = clf.feature_importances_\n",
    "            fi = pd.DataFrame(fi).transpose()\n",
    "            fi.columns = X.columns\n",
    "            fi = fi.melt()\n",
    "            fi = fi.sort_values('value', ascending=False) \n",
    "            if fi_data is None:\n",
    "                fi_data = fi            \n",
    "            else:\n",
    "                fi_data = pd.merge(fi_data, fi, on='variable')  \n",
    "    # Make prediction\n",
    "        if len(second_df) > 0:\n",
    "            probability = clf.predict_proba(second_df.drop(instance, axis=1))[:, 1]\n",
    "            prediction['probability_'+str(count)] = probability\n",
    "        else:\n",
    "            probability = clf.predict_proba(df.drop(instance, axis=1))[:, 1]  \n",
    "            prediction['probability_'+str(count)] = probability        \n",
    "    # Removing prediction that were used in the train test set.\n",
    "        if remove_training:\n",
    "            for i in index:\n",
    "                prediction.loc[i, 'probability_'+str(count)] = np.nan        \n",
    "        count += 1\n",
    "    if feature_imp:\n",
    "        return prediction , fi_data\n",
    "    else:\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e292e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the names here of proteins that fulfill a certain criteria \n",
    "# Here we use a list of proteins that that form liquid condensates in vitro and in vivo\n",
    "uniprot_ids = '''Q99700\n",
    "O60885\n",
    "Q14781\n",
    "P45973\n",
    "P38432\n",
    "Q7Z5Q1\n",
    "O00571\n",
    "Q9NQI0\n",
    "O43781\n",
    "Q04637\n",
    "Q15056\n",
    "P15502\n",
    "Q01844\n",
    "P22087\n",
    "Q06787\n",
    "P35637\n",
    "Q13283\n",
    "Q9UN86\n",
    "P10071\n",
    "P62993\n",
    "Q13151\n",
    "P09651\n",
    "Q32P51\n",
    "P22626\n",
    "P51991\n",
    "O14979\n",
    "P31943\n",
    "P55795\n",
    "P31942\n",
    "O43561\n",
    "P10636\n",
    "P43243\n",
    "Q15648\n",
    "P19338\n",
    "P06748\n",
    "Q15233\n",
    "P52948\n",
    "Q01860\n",
    "P11940\n",
    "P29590\n",
    "Q8WXF1\n",
    "Q96PK6\n",
    "P98179\n",
    "P23246\n",
    "Q16637\n",
    "P00441\n",
    "Q07889\n",
    "P23497\n",
    "Q07955\n",
    "Q01130\n",
    "O95793\n",
    "O75683\n",
    "Q92804\n",
    "Q13148\n",
    "Q15554\n",
    "P31483\n",
    "Q01085\n",
    "Q9UHD9\n",
    "P46937\n",
    "Q15059\n",
    "P10644\n",
    "P54727\n",
    "Q13501\n",
    "Q9NPI6\n",
    "O00444\n",
    "Q53HL2\n",
    "Q9ULW0\n",
    "Q9Y6A5\n",
    "Q96LT7\n",
    "P06748\n",
    "Q16236\n",
    "O43823\n",
    "Q07157\n",
    "Q9UDY2\n",
    "O95049\n",
    "Q9Y6M1\n",
    "Q6NT89\n",
    "Q8NC56\n",
    "P04150\n",
    "Q9BYJ9\n",
    "Q9Y5A9\n",
    "Q7Z739\n",
    "P10997\n",
    "Q9GZV5\n",
    "P51608\n",
    "O14979\n",
    "P43351\n",
    "Q08379\n",
    "P08621\n",
    "Q15424\n",
    "Q16630\n",
    "P18615\n",
    "P48443\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134250c7",
   "metadata": {},
   "source": [
    "# Input files, annotate and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9dbb4494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path to file with sequences of interest to be predicted. \n",
    "# This file should be conform the fasta proteome format (e.g. something like below)\n",
    "# >sp|Q96R72|OR4K3_HUMAN Olfactory receptor 4K3 OS=Homo sapiens OX=9606 GN=OR4K3 PE=3 SV=3\n",
    "# FASTA SEQUENCE\n",
    "# or\n",
    "# >tr|QID2|name\n",
    "# FASTA SEQUENCE\n",
    "proteome_path= \"/path/to/sequences.fa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ccd32d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this step the input sequences of interest get annotated for amino acid content and related features. \n",
    "# Simultaneously, as .pkl file is generated and saved in the directory that has these features and can be loaded back in\n",
    "# the notebooks (for example in the visualization notebook)\n",
    "second_df = main(name='proteome_of_interest', fasta_path=proteome_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc16877d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file with amino acid features as .csv if needed\n",
    "# second_df.to_csv('path/of/interest/filename.csv')\n",
    "# if needed this df can also be saved using\n",
    "# compressed_pickle('output_name', second_df)\n",
    "# second_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6e520211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we annotate the llps proteins in the df if they were overlapping with the original training set. \n",
    "# This overlap is likely very low so you'll likely get a lot of messages saying that protein X and Y are not found. This \n",
    "# can be ignored. \n",
    "second_df = set_identifiers(second_df, uniprot_ids, identifier_name = 'llps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "71db3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do some final preprocessing on the data frame with sequences of interest\n",
    "df2 = preprocess_and_scaledata(second_df, instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1397b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load : get the data from file that was used to train the RF model\n",
    "data = decompress_pickle('/path/to/RF_original_input_data.pbz2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3602ad2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19504, 96)\n",
      "Number of phase separating proteins in dataset: 90\n",
      "(19504, 96)\n"
     ]
    }
   ],
   "source": [
    "# Data filtering to make sure that very small (<100 AAs) and very big (> 3000 AAs) proteins are removed\n",
    "# Just some other checks on the data that certain columns are not present\n",
    "instance = 'llps'\n",
    "data = data.loc[(data['length'] >= 100) & (data['length'] <= 3000)]\n",
    "data = data.reset_index(drop=True)\n",
    "domain_cols = data.columns[data.columns.str.contains(\"^domain\")]\n",
    "domain_data = data[domain_cols]\n",
    "data.drop(columns=domain_cols)\n",
    "data = preprocess_and_scaledata(data, instance)\n",
    "label = 'llps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3945ed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained RF model\n",
    "clf = pickle.load(open('/path/to/finalized_RF.sav', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0b98cf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 216/216 [22:31<00:00,  6.26s/it]\n"
     ]
    }
   ],
   "source": [
    "# now predict the sequences of interest. INPUT PARAMETERS:\n",
    "# data = data (used to train the original classifier)\n",
    "# clf = pretrained RF model\n",
    "# second_df = df2 (which contains the sequences of interest)\n",
    "prediction, fi_data = predict_proteome(data,\n",
    "                                       clf,\n",
    "                                       instance='llps',\n",
    "                                       testing_size=.2,\n",
    "                                       # remove_training=True,\n",
    "                                       second_df=df2,\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d27df2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The prediction will contain 216 columns with prediction values (from 216 RF predictions). \n",
    "# Take the average of these values to get the PSAP score per sequence of interest\n",
    "\n",
    "# save predictions to .csv file\n",
    "# prediction.to_csv('path/of/interest/predicted_values.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "97d27187",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.to_csv('predicted_values.csv', sep ='\\t')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
